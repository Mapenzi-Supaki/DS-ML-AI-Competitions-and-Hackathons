{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "interpreter": {
   "hash": "4b49dac480128e3353893aae5d79bc0c223b3b150e1bd621885640ef2984f829"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Import Libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suppress FutureWarnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "\n",
    "#display the graphics made by python inline with the text\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "source": [
    "## Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Train.csv')\n",
    "test = pd.read_csv('Test.csv')\n",
    "submission = pd.read_csv('SampleSubmission.csv')"
   ]
  },
  {
   "source": [
    "## Understand the Data with Descriptive Statistics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#Check the shape and size of datasets\n",
    "train.shape, test.shape, submission.shape"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "#peek at the data\n",
    "train.tail()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "#Look at first 5 records\n",
    "test.head()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "submission.head()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistical summary\n",
    "train.describe()"
   ]
  },
  {
   "source": [
    "Here:\n",
    "- 0: Did not use mobile or internet banking\n",
    "- 1: Used mobile or internet banking\n",
    "\n",
    "This shows that less people use moble or internet banking."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Examine Missing & Duplicated Values\n",
    "Data\n",
    "There are also various ways to handle missing data:\n",
    " - Remove any row with missing data\n",
    " - Remove any column with missing data\n",
    " - Impute missing values\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Duplicated values in data sets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#Find duplicates\n",
    "train.duplicated().any()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.duplicated().any()"
   ]
  },
  {
   "source": [
    "### Missing Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#Counting the Number of Null rows in each Column of the dataframe\n",
    "train.isnull().sum()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "#Counting the Number of Null rows in each Column of the dataframe\n",
    "test.isnull().sum()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Dealing with Missing Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total missing values for each feature\n",
    "print (train.isnull().sum()/ len(train)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = train.isnull().sum().sort_values(ascending=False)\n",
    "percent =total/len(train)*100\n",
    "pd.concat([total,percent], axis=1, keys=['Total','Percent']).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (test.isnull().sum()/ len(test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test = test.isnull().sum().sort_values(ascending=False)\n",
    "percent =total_test/len(train)*100\n",
    "pd.concat([total,percent], axis=1, keys=['Total','Percent']).head(20)"
   ]
  },
  {
   "source": [
    "Here you can clearly see that 7 columns have Null values higher than 80% so it is good to drop those columns from our data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "train = train [train.columns[train.isnull().mean() < 0.80]]\n",
    "train.columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "source": [
    "train = train.dropna(thresh = 2)                # Apply dropna() function\n",
    "train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "test = test.dropna(thresh = 2)                # Apply dropna() function\n",
    "test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(subset = ['FQ33'], axis = 0, how = 'any', inplace = True)\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dropna(subset = ['FQ33'], axis = 0, how = 'any', inplace = True)\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop (['ID'], axis = 1)\n",
    "test = test.drop(['ID'], axis = 1)\n",
    "#test = test.drop(['ID','FQ5', 'FQ17', 'FQ36', 'FQ27', 'FQ28', 'FQ30', 'FQ31'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "source": [
    "## Train the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)\n",
    "X.head()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.Target\n",
    "X = train.drop('Target', axis=1)\n",
    "#X = X.select_dtypes(exclude=['object'])\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "# Impute training and test data\n",
    "imputer = SimpleImputer(missing_values=np.nan)\n",
    "#fit imputer to train data\n",
    "X = imputer.fit_transform(X)\n",
    "test = imputer.transform(test)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Scale training and test data\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "source": [
    "##Splitting data for training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Break off validation set from training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#rfc= RandomForestClassifier(n_estimators= 500)\n",
    "rfc= RandomForestClassifier(n_estimators= 300)\n",
    "#rfm= RGFClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "#Predictions\n",
    "y_predict = rfc.predict(X_test)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting data for training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Break off validation set from training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=999)"
   ]
  },
  {
   "source": [
    "#Make prediction on test data and submission file\n",
    "\n",
    "rfc.fit(X,y)\n",
    "\n",
    "test_pred = rfc.predict(test)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "submission['Target'] = test_pred"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "submission.head()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "submission.to_csv('submission_RFC.csv', index=False)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=10,\n",
    "                               n_jobs = -1)\n",
    "scorer = make_scorer(f1_score, greater_is_better=True, average = 'macro')\n",
    "# 10 fold cross validation\n",
    "cv_score = cross_val_score(rf_model, X_train, y_train, cv=10, scoring=scorer)\n",
    "print('10 Fold Cross Validation F1 Score = {} with std = {}'.format(round(cv_score.mean(), 4), round(cv_score.std(), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of training data, fit model \n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['Target'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_LGBM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=10,\n",
    "                               n_jobs = -1)\n",
    "scorer = make_scorer(f1_score, greater_is_better=True, average = 'macro')\n",
    "# 10 fold cross validation\n",
    "cv_score = cross_val_score(model, trainData, trainTarget, cv=10, scoring=scorer)\n",
    "print('10 Fold Cross Validation F1 Score = {} with std = {}'.format(round(cv_score.mean(), 4), round(cv_score.std(), 4)))"
   ]
  },
  {
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "#define imputer\n",
    "imputer = IterativeImputer()\n",
    "#fit on the dataset\n",
    "imputer.fit(X)\n",
    "#transform the dataset\n",
    "X_imputed = imputer.transform(X)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "X_train = pd.get_dummies(X_train)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "test = pd.get_dummies(test)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan)\n",
    "X_imputed = imputer.fit_transform(X_train.values)\n",
    "imputed_X_test = imputer.transform(X_test.values)\n",
    "print(\"Imputed data:\")\n",
    "print(X_imputed)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan)\n",
    "\n",
    "imputed_X_train = pd.DataFrame(imputer.fit_transform(X_train))\n",
    "imputed_X_test = pd.DataFrame(imputer.transform(X_test))\n",
    "imputed_test = pd.DataFrame(imputer.transform(test))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_test.columns = X_test.columns\n",
    "imputed_test.columns = test.columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Number of missing values in each column of training data\n",
    "missing_val_count_by_column = (imputed_X_train.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "print(\"MAE from Approach 2 (Imputation):\")\n",
    "print((imputed_X_train, imputed_X_test, y_train, y_test))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "#define imputer\n",
    "imputer = IterativeImputer()\n",
    "#fit on the dataset\n",
    "imputer.fit(X_train)\n",
    "#transform the dataset\n",
    "X_trainB = imputer.transform(X_train)\n",
    "X_testB = imputer.transform(X_test)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = my_imputer.fit_transform(X_train)\n",
    "imputed_X_test = my_imputer.transform(X_test)\n",
    "print(\"Mean Absolute Error from Imputation:\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#Itlearns about the data and does nothing else\n",
    "my_imputer.fit(X_train)\n",
    "\n",
    "#Calling transform to apply the learnt information on supplied data\n",
    "X_train_new = my_imputer.transform(X_train)\n",
    "X_test_new = my_imputer.transform(X_test)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "\n",
    "\n",
    "X_imputed = imputer.fit_transform(dfstd['marks'].values.reshape(-1,1))[:,0]\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "X = pd.DataFrame(imputer.fit_transform(X),columns = X.columns)\n",
    "X.head()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "#Initialize KNNImputer\n",
    "imputer = KNNImputer (n_neighbors = 2)\n",
    "\n",
    "#Impute/Fill Missing values of each feature\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform (X_train, y_train),  columns= X.columns)\n",
    "\n",
    "results = imputer.transform (X_test)\n",
    "\n",
    "results.shape\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "#Initialize KNNImputer\n",
    "imputer = KNNImputer (n_neighbors = 2)\n",
    "\n",
    "#Impute/Fill Missing values of each feature\n",
    "X_imputed = imputer.fit_transform (X)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Model Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputer = SimpleImputer()\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline ([('impute', imputer), ('model', dt_model)])\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=1)\n",
    "\n",
    "'Mean accurracy: {} std: {}'.format(round(np.mean(scores), 3), round(np.std(scores), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of training data, fit model \n",
    "pipeline.fit(imputed_X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "predictions = pipeline.predict(imputed_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of missing values in each column of training data\n",
    "missing_val_count_by_column = (imputed_X_test.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['Target'] = predictions"
   ]
  },
  {
   "source": [
    "submission = pd.DataFrame({'Target':predictions})"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "source": [
    "output = pd.DataFrame({'ID': test.ID, 'Target': predictions})"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "my_model = XGBRegressor()\n",
    "my_model.fit(X_train, y_train)\n",
    "#validate model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "predictions = my_model.predict(X_valid)\n",
    "print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('submission_Tree.csv', index=False)"
   ]
  },
  {
   "source": [
    "## Make new prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_imputed.shape)\n",
    "print(y_train.shape)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#Instantiate model and fit to data\n",
    "dt_model.fit(X, y)\n",
    "#Make predictions and store in 'Survived' column of df_test\n",
    "Y_pred = dt_model.predict(imputed_X_test)\n",
    "test['Survived'] = Y_pred"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "dt_model.fit(X_imputed, y_train)\n",
    "predictions = dt_model.predict(imputed_X_test)\n",
    "\n",
    "test['Target'] = predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "dt_model = dt_model.fit(X_imputed, y)\n",
    "y_predict = dt_model.predict(imputed_X_test)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Make predictions and store in 'Survived' column of df_test\n",
    "test['Target'] = y_predict"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "df.apply(lambda col: col.drop_duplicates().reset_index(drop=True))\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape, y_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ID = test['ID']\n",
    "submission_df = pd.DataFrame({\n",
    "                  \"ID\": sub_ID, \n",
    "                  \"Target\": y_predict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':predictions})\n",
    "\n",
    "#Visualize the first 5 rows\n",
    "submission.head("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID = test['ID']\n",
    "submission_df = pd.DataFrame({\n",
    "                  \"ID\": test.ID, \n",
    "                  \"Target\": y_predict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(imputed_X_train, y_train)\n",
    "y_pred = clf.predict(imputed_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df_1.to_csv('submission_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr = xgb.XGBRegressor()\n",
    "xgbr = xgbr.fit(X, y)\n",
    "pred_values = xgbr.predict(test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model = dt_model.fit (X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model score: %.3f\" % dt_model.score(X_testB, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.4\n",
    "\n",
    "predicted_proba = dt_model.predict_proba(X_testB)\n",
    "predicted = (predicted_proba [:,1] >= threshold).astype('int')\n",
    "\n",
    "accuracy = f1_score(y_test, predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred= dt_model.predict(X_testB)\n",
    "print('Score', f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred= dt_model.predict(X_testB)\n",
    "print('Score', f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.4\n",
    "\n",
    "predicted_proba = dt_model.predict_proba(X_test)\n",
    "predicted = (predicted_proba [:,1] >= threshold).astype('int')\n",
    "\n",
    "accuracy = f1_score(y_test, predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred= rf.predict(val_x)\n",
    "print('Score', f1_score(val_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(val_y, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = clf.predict (X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.drop(['ID', 'FQ5', 'FQ17', 'FQ36', 'FQ27', 'FQ28', 'FQ30', 'FQ31'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=2)\n",
    "rf_model.fit(X_imputed, y)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'ID': test.ID, 'Target': y_pred})\n",
    "output.to_csv('submission_RF2.csv', index=False)"
   ]
  },
  {
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train = pd.DataFrame(scaler.fit_transform(train), columns = train.columns)\n",
    "train.head()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Imputing Missing Data Using Sklearn SimpleImputer\n",
    "SimpleImputer is a class found in package sklearn.impute. It is used to impute / replace the numerical or categorical missing data related to one or more features with appropriate values. \n",
    "https://dzone.com/articles/imputing-missing-data-using-sklearn-simpleimputer\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "There are two columns / features (one numerical - marks, and another categorical - gender) which are having missing values and need to be imputed. In the code below, an instance of SimpleImputer is created with strategy as \"mean\". The missing value is represented using NaN. Note some of the following:\n",
    "\n",
    "sklearn.impute package is used for importing SimpleImputer class.\n",
    "SimpleImputer takes two argument such as missing_values and strategy.\n",
    "fit_transform method is invoked on the instance of SimpleImputer to impute the missing values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Imputation Approach with KNNImputer\n",
    "We will use the KNNImputer function from the impute module of the sklearn. KNNImputer helps to impute missing values present in the observations by finding the nearest neighbors with the Euclidean distance matrix."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "#KNN based imputation for categorical variables\n",
    "imputer = KNNImputer (n_neighbors = 2)\n",
    "\n",
    "\n",
    "imputed_train = imputer.fit_transform(train[['country_code', 'region', 'age', 'FQ1', 'FQ2', 'FQ3', 'FQ4','FQ5', 'FQ6', 'FQ7', 'FQ8', 'FQ9', 'FQ10', 'FQ11', 'FQ12', 'FQ13','FQ14', 'FQ15', 'FQ16', 'FQ17', 'FQ18', 'FQ19', 'FQ20', 'FQ21', 'FQ22','FQ23', 'FQ24', 'FQ35', 'FQ36', 'FQ25', 'FQ26', 'FQ27', 'FQ28', 'FQ29','FQ30', 'FQ31', 'FQ32', 'FQ33', 'FQ34', 'FQ37']])\n",
    "\n",
    "#print the completed dataframe\n",
    "imputed_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the SimpleImputer class\n",
    "from sklearn.impute import SimpleImputer\n",
    "  \n",
    "# Imputer object using the mean strategy and \n",
    "# missing_values type for imputation\n",
    "imputer = SimpleImputer(missing_values = np.nan, \n",
    "                        strategy ='mean')\n",
    "  \n",
    "# Fitting the data to the imputer object\n",
    "imputer = imputer.fit(train)\n",
    "  \n",
    "# Imputing the data     \n",
    "imputed_train = imputer.transform(train)\n",
    "  \n",
    "print(\"Imputed Data : \\n\", imputed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Missing values is represented using NaN and hence specified. If it is empty field, missing values will be specified as:\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "\n",
    "dfstd.marks = imputer.fit_transform(dfstd['marks'].values.reshape(-1,1))[:,0]\n",
    "\n",
    "dfstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "To drop all rows with 'any' NAs in a particular column, I used .dropna() and specified the subset = column."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(subset = ['FQ33'], axis = 0, how = 'any', inplace = True)\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=\"cabin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class Distribution\n",
    "target_counts = train.groupby('Target').size()\n",
    "print(target_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "You can see that there are nearly triple the number of observations with target 0 than there are with target 1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Examine Target Column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sns.countplot(x = 'Target',data = train)\n",
    "sizes=[]\n",
    "for p in s.patches:\n",
    "    height = p.get_height()\n",
    "    sizes.append(height)\n",
    "    s.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/len(train)*100),\n",
    "            ha=\"center\", fontsize=14) "
   ]
  },
  {
   "source": [
    "## Create Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "sns.pairplot(train, hue='Target', size=1.5);"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first three rows\n",
    "train.iloc[1:4]"
   ]
  },
  {
   "source": [
    "## Replace Multiple Values in Multiple Columns\n",
    "\n",
    "Target Category:\n",
    "- 1: Yes\n",
    "- 2: No\n",
    "- 3: Don’t know \n",
    "- 4: Refused to answer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "train.count()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "train.replace({'FQ1': {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ2' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ3' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ4' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'},\n",
    "'FQ5' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ6'{1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'},\n",
    "'FQ7' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'},\n",
    "'FQ8' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'},\n",
    "'FQ9' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ10' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ11'{1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ12' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ13'{1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'},\n",
    "'FQ14' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ15' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ16' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ17' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ18' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ19' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ20' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ21' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ22' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'},\n",
    "'FQ23' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ24' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ35' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ36'{1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ25'{1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ26' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ27' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ28' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ29' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'},\n",
    "'FQ30'{1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ31' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ32' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ33' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ34' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}, \n",
    "'FQ37' {1: 'Yes', 2: 'No', 3: 'Don’t know', 4: 'Refused to answer'}})"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "train.columns"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select unique values\n",
    "train['country_code'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, value_counts will display all unique values with the number of times each value appears:\n",
    "# Show counts\n",
    "train['country_code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select unique values\n",
    "train['region'].value_counts()"
   ]
  },
  {
   "source": [
    "Both unique and value_counts are useful for manipulating and exploring categorical\n",
    "columns."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['FQ1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['FQ4'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change multiple columns with float to int\n",
    "train[['FQ1', 'FQ2', 'FQ3', 'FQ4',\n",
    "       'FQ5', 'FQ6', 'FQ7', 'FQ8', 'FQ9', 'FQ10', 'FQ11', 'FQ12', 'FQ13',\n",
    "       'FQ14', 'FQ15', 'FQ16', 'FQ17', 'FQ18', 'FQ19', 'FQ20', 'FQ21', 'FQ22',\n",
    "       'FQ23', 'FQ24', 'FQ35', 'FQ36', 'FQ25', 'FQ26', 'FQ27', 'FQ28', 'FQ29',\n",
    "       'FQ30', 'FQ31', 'FQ32', 'FQ33', 'FQ34', 'FQ37']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}